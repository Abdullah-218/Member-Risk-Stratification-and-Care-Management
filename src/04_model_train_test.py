# src/04_model_train_test.py

"""
COMPREHENSIVE MODEL COMPARISON & OPTIMIZATION
==============================================

Improvements Applied:
1. Model Comparison: XGBoost, Random Forest, LightGBM, CatBoost
2. Threshold Optimization: ROI-maximizing thresholds
3. Cost-Sensitive Learning: Optimized class weights
4. Top-K Evaluation: Recall@Top10%, Precision@Top10%
5. Calibration: Probability calibration for better risk scores

Focus: Business impact, clinical relevance, interpretability
"""

import pandas as pd
import numpy as np
import joblib
from pathlib import Path

# Models
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# Metrics & Calibration
from sklearn.metrics import (
    classification_report, roc_auc_score, roc_curve,
    precision_recall_curve, average_precision_score,
    accuracy_score, precision_score, recall_score
)
from sklearn.calibration import CalibratedClassifierCV

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

class AdvancedModelComparison:
    """
    Compare multiple models with business-aligned evaluation
    """
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.models = {}
        self.results = {}
        self.best_models = {}
        
        # Create output directory
        Path('data/output/comparison').mkdir(parents=True, exist_ok=True)
    
    def load_data(self):
        """Load train/test data"""
        print("="*70)
        print("LOADING DATA")
        print("="*70)
        
        self.X_train = pd.read_csv('data/processed/X_train.csv')
        self.X_test = pd.read_csv('data/processed/X_test.csv')
        
        self.y_30_train = pd.read_csv('data/processed/y_30_train.csv').values.ravel()
        self.y_30_test = pd.read_csv('data/processed/y_30_test.csv').values.ravel()
        
        self.y_60_train = pd.read_csv('data/processed/y_60_train.csv').values.ravel()
        self.y_60_test = pd.read_csv('data/processed/y_60_test.csv').values.ravel()
        
        self.y_90_train = pd.read_csv('data/processed/y_90_train.csv').values.ravel()
        self.y_90_test = pd.read_csv('data/processed/y_90_test.csv').values.ravel()
        
        print(f"  ‚úÖ Train: {self.X_train.shape}")
        print(f"  ‚úÖ Test: {self.X_test.shape}\n")
    
    def get_model_configs(self, y_train):
        """Get optimized model configurations"""
        
        # Calculate class weight
        scale_pos = (y_train == 0).sum() / (y_train == 1).sum()
        
        configs = {
            'XGBoost': XGBClassifier(
                max_depth=6,
                learning_rate=0.05,
                n_estimators=300,
                subsample=0.8,
                colsample_bytree=0.8,
                scale_pos_weight=scale_pos,
                random_state=self.random_state,
                n_jobs=-1,
                eval_metric='logloss'
            ),
            
            'RandomForest': RandomForestClassifier(
                n_estimators=300,
                max_depth=12,
                min_samples_split=20,
                min_samples_leaf=10,
                class_weight='balanced',
                random_state=self.random_state,
                n_jobs=-1
            ),
            
            'LightGBM': LGBMClassifier(
                n_estimators=300,
                max_depth=8,
                learning_rate=0.05,
                num_leaves=31,
                subsample=0.8,
                colsample_bytree=0.8,
                scale_pos_weight=scale_pos,
                random_state=self.random_state,
                n_jobs=-1,
                verbose=-1
            ),
            
            'CatBoost': CatBoostClassifier(
                iterations=300,
                depth=6,
                learning_rate=0.05,
                scale_pos_weight=scale_pos,
                random_state=self.random_state,
                verbose=False
            ),
            
            'ExtraTrees': ExtraTreesClassifier(
                n_estimators=300,
                max_depth=12,
                min_samples_split=20,
                min_samples_leaf=10,
                class_weight='balanced',
                random_state=self.random_state,
                n_jobs=-1
            )
        }
        
        return configs
    
    def calculate_top_k_metrics(self, y_true, y_pred_proba, k_pct=10):
        """Calculate Recall and Precision at Top K%"""
        
        # Top K% threshold
        threshold = np.percentile(y_pred_proba, 100 - k_pct)
        top_k_mask = y_pred_proba >= threshold
        
        # Metrics
        if top_k_mask.sum() == 0:
            return 0.0, 0.0
        
        recall = y_true[top_k_mask].sum() / y_true.sum() if y_true.sum() > 0 else 0
        precision = y_true[top_k_mask].sum() / top_k_mask.sum() if top_k_mask.sum() > 0 else 0
        
        return recall, precision
    
    def find_optimal_threshold(self, y_true, y_pred_proba, cost_intervention=500, 
                               cost_missed=10000, savings_prevented=8000):
        """
        Find ROI-maximizing threshold
        
        Logic:
        - True Positive: -cost_intervention + savings_prevented
        - False Positive: -cost_intervention
        - True Negative: 0
        - False Negative: -cost_missed
        """
        
        thresholds = np.arange(0.1, 0.9, 0.01)
        best_roi = -np.inf
        best_threshold = 0.5
        
        for thresh in thresholds:
            y_pred = (y_pred_proba >= thresh).astype(int)
            
            tp = ((y_pred == 1) & (y_true == 1)).sum()
            fp = ((y_pred == 1) & (y_true == 0)).sum()
            tn = ((y_pred == 0) & (y_true == 0)).sum()
            fn = ((y_pred == 0) & (y_true == 1)).sum()
            
            # ROI calculation
            roi = (
                tp * (-cost_intervention + savings_prevented) +
                fp * (-cost_intervention) +
                tn * 0 +
                fn * (-cost_missed)
            )
            
            if roi > best_roi:
                best_roi = roi
                best_threshold = thresh
        
        return best_threshold, best_roi
    
    def train_and_evaluate_window(self, window_name, y_train, y_test):
        """Train all models for a specific window"""
        
        print(f"\n{'='*70}")
        print(f"TRAINING MODELS: {window_name.upper()}")
        print(f"{'='*70}")
        print(f"Positive rate: Train={y_train.mean()*100:.1f}%, Test={y_test.mean()*100:.1f}%\n")
        
        configs = self.get_model_configs(y_train)
        window_results = []
        
        for model_name, model in configs.items():
            print(f"  Training {model_name}...")
            
            # Train
            model.fit(self.X_train, y_train)
            
            # Calibrate
            calibrated_model = CalibratedClassifierCV(
                model, method='isotonic', cv='prefit'
            )
            calibrated_model.fit(self.X_train, y_train)
            
            # Predictions
            y_pred_proba = calibrated_model.predict_proba(self.X_test)[:, 1]
            
            # Find optimal threshold
            optimal_thresh, optimal_roi = self.find_optimal_threshold(
                y_test, y_pred_proba
            )
            y_pred_optimal = (y_pred_proba >= optimal_thresh).astype(int)
            
            # Standard metrics
            auc = roc_auc_score(y_test, y_pred_proba)
            avg_precision = average_precision_score(y_test, y_pred_proba)
            
            # Optimal threshold metrics
            accuracy = accuracy_score(y_test, y_pred_optimal)
            precision = precision_score(y_test, y_pred_optimal, zero_division=0)
            recall = recall_score(y_test, y_pred_optimal)
            
            # Top-K metrics
            recall_top10, precision_top10 = self.calculate_top_k_metrics(y_test, y_pred_proba, k_pct=10)
            recall_top20, precision_top20 = self.calculate_top_k_metrics(y_test, y_pred_proba, k_pct=20)
            
            # Store results
            result = {
                'model': model_name,
                'window': window_name,
                'auc': auc,
                'avg_precision': avg_precision,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'recall_top10': recall_top10,
                'precision_top10': precision_top10,
                'recall_top20': recall_top20,
                'precision_top20': precision_top20,
                'optimal_threshold': optimal_thresh,
                'optimal_roi': optimal_roi,
                'calibrated_model': calibrated_model,
                'y_pred_proba': y_pred_proba
            }
            
            window_results.append(result)
            
            print(f"    ‚úì AUC: {auc:.4f} | Accuracy: {accuracy:.2%} | "
                  f"Recall@Top10%: {recall_top10:.2%} | Precision@Top10%: {precision_top10:.2%}")
        
        return window_results
    
    def train_all_models(self):
        """Train models for all windows"""
        
        all_results = []
        
        # 30-day window
        results_30 = self.train_and_evaluate_window('30_day', self.y_30_train, self.y_30_test)
        all_results.extend(results_30)
        
        # 60-day window
        results_60 = self.train_and_evaluate_window('60_day', self.y_60_train, self.y_60_test)
        all_results.extend(results_60)
        
        # 90-day window
        results_90 = self.train_and_evaluate_window('90_day', self.y_90_train, self.y_90_test)
        all_results.extend(results_90)
        
        self.results_df = pd.DataFrame([
            {k: v for k, v in r.items() if k not in ['calibrated_model', 'y_pred_proba']}
            for r in all_results
        ])
        
        self.all_results = all_results
    
    def select_best_models(self):
        """Select best model for each window based on business metrics"""
        
        print(f"\n{'='*70}")
        print("SELECTING BEST MODELS (Business-Aligned)")
        print(f"{'='*70}\n")
        
        # Scoring function: weighted combination of metrics
        def business_score(row):
            return (
                row['recall_top10'] * 0.4 +  # Capture high-risk patients
                row['precision_top10'] * 0.3 +  # Minimize false alarms
                row['auc'] * 0.2 +  # Overall discrimination
                row['recall'] * 0.1  # General recall
            )
        
        self.results_df['business_score'] = self.results_df.apply(business_score, axis=1)
        
        for window in ['30_day', '60_day', '90_day']:
            window_results = self.results_df[self.results_df['window'] == window]
            best_idx = window_results['business_score'].idxmax()
            best_result = window_results.loc[best_idx]
            
            print(f"  {window}:")
            print(f"    üèÜ Winner: {best_result['model']}")
            print(f"    Business Score: {best_result['business_score']:.4f}")
            print(f"    AUC: {best_result['auc']:.4f}")
            print(f"    Recall@Top10%: {best_result['recall_top10']:.2%}")
            print(f"    Precision@Top10%: {best_result['precision_top10']:.2%}")
            print(f"    Optimal Threshold: {best_result['optimal_threshold']:.3f}\n")
            
            # Save best model
            best_model_obj = [r for r in self.all_results 
                            if r['model'] == best_result['model'] and r['window'] == window][0]
            
            self.best_models[window] = {
                'model': best_model_obj['calibrated_model'],
                'name': best_result['model'],
                'threshold': best_result['optimal_threshold'],
                'metrics': best_result.to_dict()
            }
    
    def plot_comparison_charts(self):
        """Generate comparison visualizations"""
        
        print(f"\n{'='*70}")
        print("GENERATING COMPARISON CHARTS")
        print(f"{'='*70}\n")
        
        # 1. Model Comparison Heatmap
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        metrics_to_plot = ['auc', 'recall_top10', 'precision_top10']
        metric_names = ['ROC-AUC', 'Recall@Top10%', 'Precision@Top10%']
        
        for idx, (metric, metric_name) in enumerate(zip(metrics_to_plot, metric_names)):
            ax = axes[idx]
            
            pivot = self.results_df.pivot(index='model', columns='window', values=metric)
            pivot = pivot[['30_day', '60_day', '90_day']]  # Order columns
            
            sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn', 
                       vmin=0, vmax=1, ax=ax, cbar_kws={'label': metric_name})
            ax.set_title(f'{metric_name} Comparison', fontsize=12, fontweight='bold')
            ax.set_xlabel('Prediction Window')
            ax.set_ylabel('Model')
        
        plt.tight_layout()
        plt.savefig('data/output/comparison/model_comparison_heatmap.png', dpi=150)
        plt.close()
        print("  ‚úÖ Saved: model_comparison_heatmap.png")
        
        # 2. Business Score Comparison
        fig, ax = plt.subplots(figsize=(12, 6))
        
        pivot_score = self.results_df.pivot(index='model', columns='window', values='business_score')
        pivot_score = pivot_score[['30_day', '60_day', '90_day']]
        
        pivot_score.plot(kind='bar', ax=ax, color=['#d32f2f', '#f57c00', '#fbc02d'])
        ax.set_title('Business Score by Model and Window', fontsize=14, fontweight='bold')
        ax.set_xlabel('Model', fontsize=12)
        ax.set_ylabel('Business Score', fontsize=12)
        ax.legend(title='Window', labels=['30-day', '60-day', '90-day'])
        ax.grid(axis='y', alpha=0.3)
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig('data/output/comparison/business_score_comparison.png', dpi=150)
        plt.close()
        print("  ‚úÖ Saved: business_score_comparison.png")
        
        # 3. ROC Curves for Best Models
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        for idx, (window, window_data) in enumerate(self.best_models.items()):
            ax = axes[idx]
            
            # Get predictions
            result = [r for r in self.all_results 
                     if r['model'] == window_data['name'] and r['window'] == window][0]
            y_pred_proba = result['y_pred_proba']
            
            if window == '30_day':
                y_true = self.y_30_test
            elif window == '60_day':
                y_true = self.y_60_test
            else:
                y_true = self.y_90_test
            
            # ROC curve
            fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
            auc = result['auc']
            
            ax.plot(fpr, tpr, linewidth=2, label=f'{window_data["name"]} (AUC={auc:.3f})')
            ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
            
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title(f'{window.replace("_", "-")} Best Model', fontweight='bold')
            ax.legend()
            ax.grid(alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('data/output/comparison/best_models_roc.png', dpi=150)
        plt.close()
        print("  ‚úÖ Saved: best_models_roc.png")
    
    def save_results(self):
        """Save comparison results and best models"""
        
        print(f"\n{'='*70}")
        print("SAVING RESULTS")
        print(f"{'='*70}\n")
        
        # Save comparison table
        self.results_df.to_csv('data/output/comparison/model_comparison_results.csv', index=False)
        print("  ‚úÖ Saved: model_comparison_results.csv")
        
        # Save best models
        for window, data in self.best_models.items():
            model_path = f'models/best_{window}_model.pkl'
            joblib.dump({
                'model': data['model'],
                'name': data['name'],
                'threshold': data['threshold'],
                'metrics': data['metrics']
            }, model_path)
            print(f"  ‚úÖ Saved: best_{window}_model.pkl ({data['name']})")
    
    def generate_executive_report(self):
        """Generate executive summary of improvements"""
        
        print(f"\n{'='*70}")
        print("GENERATING EXECUTIVE REPORT")
        print(f"{'='*70}\n")
        
        report = []
        report.append("="*70)
        report.append("MODEL COMPARISON & OPTIMIZATION - EXECUTIVE SUMMARY")
        report.append("="*70)
        report.append("")
        
        report.append("METHODOLOGY")
        report.append("-"*70)
        report.append("Models Evaluated: XGBoost, Random Forest, LightGBM, CatBoost, ExtraTrees")
        report.append("Optimizations Applied:")
        report.append("  1. Cost-sensitive learning (balanced class weights)")
        report.append("  2. Probability calibration (isotonic regression)")
        report.append("  3. ROI-maximizing threshold optimization")
        report.append("  4. Top-K business metrics (Recall/Precision @ Top 10%)")
        report.append("")
        
        report.append("BEST MODELS SELECTED")
        report.append("-"*70)
        
        for window in ['30_day', '60_day', '90_day']:
            data = self.best_models[window]
            m = data['metrics']
            
            report.append(f"\n{window.replace('_', '-').upper()}:")
            report.append(f"  Model: {data['name']}")
            report.append(f"  ROC-AUC: {m['auc']:.4f}")
            report.append(f"  Accuracy: {m['accuracy']:.2%}")
            report.append(f"  Recall @ Top 10%: {m['recall_top10']:.2%}")
            report.append(f"  Precision @ Top 10%: {m['precision_top10']:.2%}")
            report.append(f"  Optimal Threshold: {m['optimal_threshold']:.3f}")
            report.append(f"  Business Score: {m['business_score']:.4f}")
        
        report.append("")
        report.append("KEY IMPROVEMENTS")
        report.append("-"*70)
        report.append("‚úì Enhanced class-balanced learning reduces missed high-risk patients")
        report.append("‚úì Calibrated probabilities provide reliable risk scores for care managers")
        report.append("‚úì Optimized thresholds maximize ROI while maintaining clinical safety")
        report.append("‚úì Top-K metrics align with real-world care management capacity")
        report.append("")
        
        report.append("CLINICAL INTERPRETATION")
        report.append("-"*70)
        report.append("‚Ä¢ Recall@Top10% shows our ability to identify truly high-risk patients")
        report.append("  in the top decile where intervention resources are deployed")
        report.append("‚Ä¢ Precision@Top10% minimizes alert fatigue from false positives")
        report.append("‚Ä¢ Calibrated risk scores enable confidence-based triage decisions")
        report.append("")
        
        report.append("="*70)
        
        # Save
        report_text = '\n'.join(report)
        with open('data/output/comparison/executive_summary.txt', 'w') as f:
            f.write(report_text)
        
        print(report_text)
        print("\n  ‚úÖ Saved: executive_summary.txt")
    
    def run_full_analysis(self):
        """Execute complete comparison pipeline"""
        self.load_data()
        self.train_all_models()
        self.select_best_models()
        self.plot_comparison_charts()
        self.save_results()
        self.generate_executive_report()

# ============================================
# MAIN EXECUTION
# ============================================

if __name__ == "__main__":
    comparison = AdvancedModelComparison()
    comparison.run_full_analysis()
    
    print("\n" + "="*70)
    print("‚úÖ MODEL COMPARISON & OPTIMIZATION COMPLETE!")
    print("="*70)
    print("\nGenerated outputs:")
    print("  üìä data/output/comparison/model_comparison_heatmap.png")
    print("  üìä data/output/comparison/business_score_comparison.png")
    print("  üìä data/output/comparison/best_models_roc.png")
    print("  üìÑ data/output/comparison/model_comparison_results.csv")
    print("  üíæ models/best_30_day_model.pkl")
    print("  üíæ models/best_60_day_model.pkl")
    print("  üíæ models/best_90_day_model.pkl")
    print("  üìÑ data/output/comparison/executive_summary.txt")
    print("\nüéØ Best models selected based on business-aligned metrics!")